{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset (assuming a CSV file)\nreviews_df = pd.read_csv(\"/kaggle/input/letterboxd-reviews-2024/cleaned_reviews.csv\")\n\n# Split dataset into training and test sets\ntrain_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:33:46.710220Z","iopub.execute_input":"2024-08-26T20:33:46.711358Z","iopub.status.idle":"2024-08-26T20:33:50.517941Z","shell.execute_reply.started":"2024-08-26T20:33:46.711296Z","shell.execute_reply":"2024-08-26T20:33:50.516801Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:34:23.622721Z","iopub.execute_input":"2024-08-26T20:34:23.623153Z","iopub.status.idle":"2024-08-26T20:34:23.648827Z","shell.execute_reply.started":"2024-08-26T20:34:23.623113Z","shell.execute_reply":"2024-08-26T20:34:23.646942Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                         review_text  rating\n0                                 monkey mondays #33     0.8\n1  I mean...it's no Pride and Prejudice (2005) bu...     0.6\n2  Addressed my inert fear of pink and pretty dre...     0.6\n3  it was good for the most part, couldnâ€™t really...     0.7\n4  Well, I'm late to the bespoke party, but this ...     0.8","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>monkey mondays #33</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I mean...it's no Pride and Prejudice (2005) bu...</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Addressed my inert fear of pink and pretty dre...</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>it was good for the most part, couldnâ€™t really...</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Well, I'm late to the bespoke party, but this ...</td>\n      <td>0.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 0. Dataset Preparation","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Convert DataFrame to Hugging Face Dataset with 'review_text' and 'rating'\ntrain_dataset = Dataset.from_pandas(train_df[['review_text', 'rating']])\ntest_dataset = Dataset.from_pandas(test_df[['review_text', 'rating']])\n\n# Rename 'rating' to 'labels'\ntrain_dataset = train_dataset.rename_column('rating', 'labels')\ntest_dataset = test_dataset.rename_column('rating', 'labels')\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['review_text'], padding=\"max_length\", truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:37:43.272375Z","iopub.execute_input":"2024-08-26T20:37:43.272930Z","iopub.status.idle":"2024-08-26T20:37:49.139669Z","shell.execute_reply.started":"2024-08-26T20:37:43.272885Z","shell.execute_reply":"2024-08-26T20:37:49.138350Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d2283b732e420fbf2ef79d1ae4271e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611fa86b071146a0af1ecc3fa454548a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e61e4de5b1c4acc8f34a328bd169b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806567df49314d37b8bcfee30e07335f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84918a62297a426bada7dff45870dda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f02f7092684ea5a66f20f68418c675"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 1. Load and Modify the Pretrained Model\n\nYou will need to modify a pretrained BERT model to fit a regression task. Hereâ€™s an example of how you can adapt BERT for regression using the Hugging Face Transformers library:","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\nimport torch\nfrom torch import nn\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # 1 for regression output\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        pooled_output = outputs[1]  # [1] corresponds to the pooled output\n        regression_output = self.regressor(pooled_output)\n\n        # For training, return loss\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(regression_output.squeeze(), labels)\n            return (loss, regression_output)\n        \n        return regression_output\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:37:55.451000Z","iopub.execute_input":"2024-08-26T20:37:55.451758Z","iopub.status.idle":"2024-08-26T20:37:56.478735Z","shell.execute_reply.started":"2024-08-26T20:37:55.451700Z","shell.execute_reply":"2024-08-26T20:37:56.477203Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertPreTrainedModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BertForRegression(BertPreTrainedModel):  # Inherit from BertPreTrainedModel\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.init_weights()  # Initialize the weights\n\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regression_head(pooled_output)\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:29:20.362992Z","iopub.execute_input":"2024-08-26T21:29:20.363437Z","iopub.status.idle":"2024-08-26T21:29:20.374435Z","shell.execute_reply.started":"2024-08-26T21:29:20.363396Z","shell.execute_reply":"2024-08-26T21:29:20.373281Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertPreTrainedModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # Output size 1 for regression\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regression_head(pooled_output)\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:29:42.056739Z","iopub.execute_input":"2024-08-26T21:29:42.057228Z","iopub.status.idle":"2024-08-26T21:29:42.067290Z","shell.execute_reply.started":"2024-08-26T21:29:42.057185Z","shell.execute_reply":"2024-08-26T21:29:42.066152Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\nimport torch.nn as nn\nimport torch\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # Output size 1 for regression\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regressor(pooled_output)  # Use self.regressor\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output to [0, 1]\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:05.415519Z","iopub.execute_input":"2024-08-26T21:33:05.415982Z","iopub.status.idle":"2024-08-26T21:33:05.426494Z","shell.execute_reply.started":"2024-08-26T21:33:05.415938Z","shell.execute_reply":"2024-08-26T21:33:05.425266Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## 2. Use the Correct Loss Function\n\nUse Mean Squared Error (MSE) for regression:","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\nimport torch\n\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get('labels')\n        outputs = model(**inputs)\n        logits = outputs[0]  # for regression model, outputs[0] is the predictions\n        \n        # Ensure labels and logits are of same shape\n        if labels is None:\n            raise ValueError(\"Labels are not provided in inputs.\")\n        \n        loss_fct = torch.nn.MSELoss()\n        loss = loss_fct(logits.squeeze(), labels)\n        \n        return (loss, outputs) if return_outputs else loss\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:08.922392Z","iopub.execute_input":"2024-08-26T21:33:08.922833Z","iopub.status.idle":"2024-08-26T21:33:08.931064Z","shell.execute_reply.started":"2024-08-26T21:33:08.922792Z","shell.execute_reply":"2024-08-26T21:33:08.929816Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get('labels')\n        outputs = model(**inputs)\n        logits = outputs[0]  # Model should return logits in outputs[0]\n        \n        # Ensure labels and logits are of the same shape\n        if labels is None:\n            raise ValueError(\"Labels are not provided in inputs.\")\n        \n        labels = labels.squeeze(-1)  # Match the dimension of logits\n        loss_fct = torch.nn.MSELoss()\n        loss = loss_fct(logits, labels)\n        \n        return (loss, outputs) if return_outputs else loss\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:09.697453Z","iopub.execute_input":"2024-08-26T21:33:09.698506Z","iopub.status.idle":"2024-08-26T21:33:09.706219Z","shell.execute_reply.started":"2024-08-26T21:33:09.698437Z","shell.execute_reply":"2024-08-26T21:33:09.704946Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## 4. Set Up and Train the Model\n\nInitialize the Trainer with your regression-specific trainer and dataset:","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nmodel_name = 'bert-base-uncased'  # Or your preferred BERT variant\nmodel = BertForRegression(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    report_to=\"none\"  \n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:40:21.247030Z","iopub.execute_input":"2024-08-26T21:40:21.247461Z","iopub.status.idle":"2024-08-26T21:40:21.645591Z","shell.execute_reply.started":"2024-08-26T21:40:21.247417Z","shell.execute_reply":"2024-08-26T21:40:21.644401Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=2,  # Use smaller batch sizes\n    per_device_eval_batch_size=2,\n    num_train_epochs=100,  # More epochs to compensate for smaller batch size\n    logging_dir='./logs',\n    logging_steps=1,  # Log more frequently\n    evaluation_strategy=\"steps\",  # Evaluate more often\n    eval_steps=1,  # Evaluate every step\n    save_steps=5,  # Save checkpoints frequently\n    save_total_limit=2,  # Keep only the last 2 checkpoints\n    report_to=\"none\",\n    learning_rate=2e-5,  # Lower learning rate for fine-tuning on small data\n    weight_decay=0.01,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Ensure this is a properly formatted dataset\n    eval_dataset=test_dataset\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:11.859490Z","iopub.execute_input":"2024-08-26T21:33:11.859971Z","iopub.status.idle":"2024-08-26T21:40:21.244671Z","shell.execute_reply.started":"2024-08-26T21:33:11.859925Z","shell.execute_reply":"2024-08-26T21:40:21.243329Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 06:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.122576</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.196600</td>\n      <td>0.059552</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.196600</td>\n      <td>0.042784</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15, training_loss=0.1582450787226359, metrics={'train_runtime': 428.7124, 'train_samples_per_second': 0.28, 'train_steps_per_second': 0.035, 'total_flos': 0.0, 'train_loss': 0.1582450787226359, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\neval_results = trainer.evaluate(eval_dataset=test_dataset)\n\n# Print evaluation results\nprint(\"Evaluation Results:\", eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:13:01.024023Z","iopub.execute_input":"2024-08-26T21:13:01.025795Z","iopub.status.idle":"2024-08-26T21:13:10.221943Z","shell.execute_reply.started":"2024-08-26T21:13:01.025671Z","shell.execute_reply":"2024-08-26T21:13:10.220766Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.012339148670434952, 'eval_runtime': 9.1825, 'eval_samples_per_second': 1.198, 'eval_steps_per_second': 0.218, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"so good, best film\"\nreview_text = \"so bad, worst film\"\n\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.squeeze().item()  # Directly access the tensor output\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:42:22.364488Z","iopub.execute_input":"2024-08-26T21:42:22.364974Z","iopub.status.idle":"2024-08-26T21:42:23.328436Z","shell.execute_reply.started":"2024-08-26T21:42:22.364927Z","shell.execute_reply":"2024-08-26T21:42:23.326999Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Predicted Rating: 0.39\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"I absolutely hated it\"\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.squeeze().item()\n    predicted_rating = max(0.0, min(1.0, predicted_rating))  # Clamp to [0, 1] range\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:27:09.817083Z","iopub.execute_input":"2024-08-26T21:27:09.818523Z","iopub.status.idle":"2024-08-26T21:27:10.749765Z","shell.execute_reply.started":"2024-08-26T21:27:09.818442Z","shell.execute_reply":"2024-08-26T21:27:10.748859Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Predicted Rating: 0.00\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"I absolutely loved this movie! The story was gripping and the acting was top-notch.\"\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.logits.squeeze().item()\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:20:31.788513Z","iopub.execute_input":"2024-08-26T21:20:31.789072Z","iopub.status.idle":"2024-08-26T21:20:33.503612Z","shell.execute_reply.started":"2024-08-26T21:20:31.789022Z","shell.execute_reply":"2024-08-26T21:20:33.501778Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m---> 13\u001b[0m     predicted_rating \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Output the predicted rating\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Rating: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_rating\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"],"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'logits'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load pre-trained BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['review_text'], padding=\"max_length\", truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:44:11.269424Z","iopub.execute_input":"2024-08-26T19:44:11.269910Z","iopub.status.idle":"2024-08-26T19:44:11.521023Z","shell.execute_reply.started":"2024-08-26T19:44:11.269862Z","shell.execute_reply":"2024-08-26T19:44:11.519927Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3caf245e5d294b40b78c293ba332c3eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0fb54783f4e43dbbb3dfccb715c506b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Load pre-trained BERT model with a regression head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:45:37.340346Z","iopub.execute_input":"2024-08-26T19:45:37.340842Z","iopub.status.idle":"2024-08-26T19:45:41.829868Z","shell.execute_reply.started":"2024-08-26T19:45:37.340788Z","shell.execute_reply":"2024-08-26T19:45:41.828257Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7d8df9a9e44e50a7aaf4301ac06e83"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    report_to=\"none\"  \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:50:18.467932Z","iopub.execute_input":"2024-08-26T19:50:18.468378Z","iopub.status.idle":"2024-08-26T19:50:18.475662Z","shell.execute_reply.started":"2024-08-26T19:50:18.468336Z","shell.execute_reply":"2024-08-26T19:50:18.474197Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:50:20.299820Z","iopub.execute_input":"2024-08-26T19:50:20.300263Z","iopub.status.idle":"2024-08-26T19:50:20.321077Z","shell.execute_reply.started":"2024-08-26T19:50:20.300221Z","shell.execute_reply":"2024-08-26T19:50:20.319683Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, BertForSequenceClassification\nimport torch\n\n# Assuming you're using a classification model:\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to your case\n\n# Define custom Trainer class with compute_loss method\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n\n        # Compute the loss (CrossEntropyLoss for classification)\n        labels = inputs.get('labels')\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:55:02.467675Z","iopub.execute_input":"2024-08-26T19:55:02.468222Z","iopub.status.idle":"2024-08-26T19:55:02.781868Z","shell.execute_reply.started":"2024-08-26T19:55:02.468174Z","shell.execute_reply":"2024-08-26T19:55:02.780658Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the Trainer with the custom loss function\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Your training dataset\n    eval_dataset=test_dataset  # Your evaluation dataset\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:04:19.194429Z","iopub.execute_input":"2024-08-26T20:04:19.194955Z","iopub.status.idle":"2024-08-26T20:04:19.212353Z","shell.execute_reply.started":"2024-08-26T20:04:19.194906Z","shell.execute_reply":"2024-08-26T20:04:19.210873Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:22:47.467830Z","iopub.execute_input":"2024-08-26T20:22:47.468155Z","iopub.status.idle":"2024-08-26T20:22:47.849977Z","shell.execute_reply.started":"2024-08-26T20:22:47.468118Z","shell.execute_reply":"2024-08-26T20:22:47.848376Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}