{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset (assuming a CSV file)\nreviews_df = pd.read_csv(\"/kaggle/input/letterboxd-reviews-2024/cleaned_reviews.csv\")\n\n# Split dataset into training and test sets\ntrain_df, test_df = train_test_split(reviews_df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:33:46.710220Z","iopub.execute_input":"2024-08-26T20:33:46.711358Z","iopub.status.idle":"2024-08-26T20:33:50.517941Z","shell.execute_reply.started":"2024-08-26T20:33:46.711296Z","shell.execute_reply":"2024-08-26T20:33:50.516801Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"reviews_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:34:23.622721Z","iopub.execute_input":"2024-08-26T20:34:23.623153Z","iopub.status.idle":"2024-08-26T20:34:23.648827Z","shell.execute_reply.started":"2024-08-26T20:34:23.623113Z","shell.execute_reply":"2024-08-26T20:34:23.646942Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                         review_text  rating\n0                                 monkey mondays #33     0.8\n1  I mean...it's no Pride and Prejudice (2005) bu...     0.6\n2  Addressed my inert fear of pink and pretty dre...     0.6\n3  it was good for the most part, couldn’t really...     0.7\n4  Well, I'm late to the bespoke party, but this ...     0.8","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>monkey mondays #33</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I mean...it's no Pride and Prejudice (2005) bu...</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Addressed my inert fear of pink and pretty dre...</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>it was good for the most part, couldn’t really...</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Well, I'm late to the bespoke party, but this ...</td>\n      <td>0.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 0. Dataset Preparation","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Convert DataFrame to Hugging Face Dataset with 'review_text' and 'rating'\ntrain_dataset = Dataset.from_pandas(train_df[['review_text', 'rating']])\ntest_dataset = Dataset.from_pandas(test_df[['review_text', 'rating']])\n\n# Rename 'rating' to 'labels'\ntrain_dataset = train_dataset.rename_column('rating', 'labels')\ntest_dataset = test_dataset.rename_column('rating', 'labels')\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['review_text'], padding=\"max_length\", truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:37:43.272375Z","iopub.execute_input":"2024-08-26T20:37:43.272930Z","iopub.status.idle":"2024-08-26T20:37:49.139669Z","shell.execute_reply.started":"2024-08-26T20:37:43.272885Z","shell.execute_reply":"2024-08-26T20:37:49.138350Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d2283b732e420fbf2ef79d1ae4271e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611fa86b071146a0af1ecc3fa454548a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e61e4de5b1c4acc8f34a328bd169b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"806567df49314d37b8bcfee30e07335f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84918a62297a426bada7dff45870dda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f02f7092684ea5a66f20f68418c675"}},"metadata":{}}]},{"cell_type":"markdown","source":"## 1. Load and Modify the Pretrained Model\n\nYou will need to modify a pretrained BERT model to fit a regression task. Here’s an example of how you can adapt BERT for regression using the Hugging Face Transformers library:","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\nimport torch\nfrom torch import nn\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # 1 for regression output\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        pooled_output = outputs[1]  # [1] corresponds to the pooled output\n        regression_output = self.regressor(pooled_output)\n\n        # For training, return loss\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(regression_output.squeeze(), labels)\n            return (loss, regression_output)\n        \n        return regression_output\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:37:55.451000Z","iopub.execute_input":"2024-08-26T20:37:55.451758Z","iopub.status.idle":"2024-08-26T20:37:56.478735Z","shell.execute_reply.started":"2024-08-26T20:37:55.451700Z","shell.execute_reply":"2024-08-26T20:37:56.477203Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertPreTrainedModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BertForRegression(BertPreTrainedModel):  # Inherit from BertPreTrainedModel\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self.init_weights()  # Initialize the weights\n\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regression_head(pooled_output)\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:29:20.362992Z","iopub.execute_input":"2024-08-26T21:29:20.363437Z","iopub.status.idle":"2024-08-26T21:29:20.374435Z","shell.execute_reply.started":"2024-08-26T21:29:20.363396Z","shell.execute_reply":"2024-08-26T21:29:20.373281Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel, BertPreTrainedModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # Output size 1 for regression\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regression_head(pooled_output)\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:29:42.056739Z","iopub.execute_input":"2024-08-26T21:29:42.057228Z","iopub.status.idle":"2024-08-26T21:29:42.067290Z","shell.execute_reply.started":"2024-08-26T21:29:42.057185Z","shell.execute_reply":"2024-08-26T21:29:42.066152Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from transformers import BertModel\nimport torch.nn as nn\nimport torch\n\nclass BertForRegression(nn.Module):\n    def __init__(self, model_name):\n        super(BertForRegression, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)  # Output size 1 for regression\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        pooled_output = outputs[\"pooler_output\"]\n        logits = self.regressor(pooled_output)  # Use self.regressor\n        logits = torch.sigmoid(logits)  # Apply sigmoid to constrain output to [0, 1]\n\n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.MSELoss()\n            loss = loss_fct(logits.squeeze(), labels)\n\n        return (loss, logits) if loss is not None else logits\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:05.415519Z","iopub.execute_input":"2024-08-26T21:33:05.415982Z","iopub.status.idle":"2024-08-26T21:33:05.426494Z","shell.execute_reply.started":"2024-08-26T21:33:05.415938Z","shell.execute_reply":"2024-08-26T21:33:05.425266Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## 2. Use the Correct Loss Function\n\nUse Mean Squared Error (MSE) for regression:","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\nimport torch\n\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get('labels')\n        outputs = model(**inputs)\n        logits = outputs[0]  # for regression model, outputs[0] is the predictions\n        \n        # Ensure labels and logits are of same shape\n        if labels is None:\n            raise ValueError(\"Labels are not provided in inputs.\")\n        \n        loss_fct = torch.nn.MSELoss()\n        loss = loss_fct(logits.squeeze(), labels)\n        \n        return (loss, outputs) if return_outputs else loss\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:08.922392Z","iopub.execute_input":"2024-08-26T21:33:08.922833Z","iopub.status.idle":"2024-08-26T21:33:08.931064Z","shell.execute_reply.started":"2024-08-26T21:33:08.922792Z","shell.execute_reply":"2024-08-26T21:33:08.929816Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.get('labels')\n        outputs = model(**inputs)\n        logits = outputs[0]  # Model should return logits in outputs[0]\n        \n        # Ensure labels and logits are of the same shape\n        if labels is None:\n            raise ValueError(\"Labels are not provided in inputs.\")\n        \n        labels = labels.squeeze(-1)  # Match the dimension of logits\n        loss_fct = torch.nn.MSELoss()\n        loss = loss_fct(logits, labels)\n        \n        return (loss, outputs) if return_outputs else loss\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:09.697453Z","iopub.execute_input":"2024-08-26T21:33:09.698506Z","iopub.status.idle":"2024-08-26T21:33:09.706219Z","shell.execute_reply.started":"2024-08-26T21:33:09.698437Z","shell.execute_reply":"2024-08-26T21:33:09.704946Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## 4. Set Up and Train the Model\n\nInitialize the Trainer with your regression-specific trainer and dataset:","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nmodel_name = 'bert-base-uncased'  # Or your preferred BERT variant\nmodel = BertForRegression(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    report_to=\"none\"  \n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:40:21.247030Z","iopub.execute_input":"2024-08-26T21:40:21.247461Z","iopub.status.idle":"2024-08-26T21:40:21.645591Z","shell.execute_reply.started":"2024-08-26T21:40:21.247417Z","shell.execute_reply":"2024-08-26T21:40:21.644401Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=2,  # Use smaller batch sizes\n    per_device_eval_batch_size=2,\n    num_train_epochs=100,  # More epochs to compensate for smaller batch size\n    logging_dir='./logs',\n    logging_steps=1,  # Log more frequently\n    evaluation_strategy=\"steps\",  # Evaluate more often\n    eval_steps=1,  # Evaluate every step\n    save_steps=5,  # Save checkpoints frequently\n    save_total_limit=2,  # Keep only the last 2 checkpoints\n    report_to=\"none\",\n    learning_rate=2e-5,  # Lower learning rate for fine-tuning on small data\n    weight_decay=0.01,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = RegressionTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Ensure this is a properly formatted dataset\n    eval_dataset=test_dataset\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:33:11.859490Z","iopub.execute_input":"2024-08-26T21:33:11.859971Z","iopub.status.idle":"2024-08-26T21:40:21.244671Z","shell.execute_reply.started":"2024-08-26T21:33:11.859925Z","shell.execute_reply":"2024-08-26T21:40:21.243329Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 06:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.122576</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.196600</td>\n      <td>0.059552</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.196600</td>\n      <td>0.042784</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15, training_loss=0.1582450787226359, metrics={'train_runtime': 428.7124, 'train_samples_per_second': 0.28, 'train_steps_per_second': 0.035, 'total_flos': 0.0, 'train_loss': 0.1582450787226359, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\neval_results = trainer.evaluate(eval_dataset=test_dataset)\n\n# Print evaluation results\nprint(\"Evaluation Results:\", eval_results)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:13:01.024023Z","iopub.execute_input":"2024-08-26T21:13:01.025795Z","iopub.status.idle":"2024-08-26T21:13:10.221943Z","shell.execute_reply.started":"2024-08-26T21:13:01.025671Z","shell.execute_reply":"2024-08-26T21:13:10.220766Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.012339148670434952, 'eval_runtime': 9.1825, 'eval_samples_per_second': 1.198, 'eval_steps_per_second': 0.218, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"so good, best film\"\nreview_text = \"so bad, worst film\"\n\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.squeeze().item()  # Directly access the tensor output\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:42:22.364488Z","iopub.execute_input":"2024-08-26T21:42:22.364974Z","iopub.status.idle":"2024-08-26T21:42:23.328436Z","shell.execute_reply.started":"2024-08-26T21:42:22.364927Z","shell.execute_reply":"2024-08-26T21:42:23.326999Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Predicted Rating: 0.39\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"I absolutely hated it\"\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.squeeze().item()\n    predicted_rating = max(0.0, min(1.0, predicted_rating))  # Clamp to [0, 1] range\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:27:09.817083Z","iopub.execute_input":"2024-08-26T21:27:09.818523Z","iopub.status.idle":"2024-08-26T21:27:10.749765Z","shell.execute_reply.started":"2024-08-26T21:27:09.818442Z","shell.execute_reply":"2024-08-26T21:27:10.748859Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Predicted Rating: 0.00\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Set the model to evaluation mode\nmodel.eval()\n\n# Your own review text\nreview_text = \"I absolutely loved this movie! The story was gripping and the acting was top-notch.\"\n\n# Tokenize the input review\ninputs = tokenizer(review_text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n# Make the prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_rating = outputs.logits.squeeze().item()\n\n# Output the predicted rating\nprint(f\"Predicted Rating: {predicted_rating:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:20:31.788513Z","iopub.execute_input":"2024-08-26T21:20:31.789072Z","iopub.status.idle":"2024-08-26T21:20:33.503612Z","shell.execute_reply.started":"2024-08-26T21:20:31.789022Z","shell.execute_reply":"2024-08-26T21:20:33.501778Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m---> 13\u001b[0m     predicted_rating \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Output the predicted rating\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Rating: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_rating\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"],"ename":"AttributeError","evalue":"'Tensor' object has no attribute 'logits'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load pre-trained BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['review_text'], padding=\"max_length\", truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:44:11.269424Z","iopub.execute_input":"2024-08-26T19:44:11.269910Z","iopub.status.idle":"2024-08-26T19:44:11.521023Z","shell.execute_reply.started":"2024-08-26T19:44:11.269862Z","shell.execute_reply":"2024-08-26T19:44:11.519927Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3caf245e5d294b40b78c293ba332c3eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0fb54783f4e43dbbb3dfccb715c506b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# Load pre-trained BERT model with a regression head\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:45:37.340346Z","iopub.execute_input":"2024-08-26T19:45:37.340842Z","iopub.status.idle":"2024-08-26T19:45:41.829868Z","shell.execute_reply.started":"2024-08-26T19:45:37.340788Z","shell.execute_reply":"2024-08-26T19:45:41.828257Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7d8df9a9e44e50a7aaf4301ac06e83"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    report_to=\"none\"  \n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:50:18.467932Z","iopub.execute_input":"2024-08-26T19:50:18.468378Z","iopub.status.idle":"2024-08-26T19:50:18.475662Z","shell.execute_reply.started":"2024-08-26T19:50:18.468336Z","shell.execute_reply":"2024-08-26T19:50:18.474197Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:50:20.299820Z","iopub.execute_input":"2024-08-26T19:50:20.300263Z","iopub.status.idle":"2024-08-26T19:50:20.321077Z","shell.execute_reply.started":"2024-08-26T19:50:20.300221Z","shell.execute_reply":"2024-08-26T19:50:20.319683Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, BertForSequenceClassification\nimport torch\n\n# Assuming you're using a classification model:\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Adjust num_labels to your case\n\n# Define custom Trainer class with compute_loss method\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n\n        # Compute the loss (CrossEntropyLoss for classification)\n        labels = inputs.get('labels')\n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-08-26T19:55:02.467675Z","iopub.execute_input":"2024-08-26T19:55:02.468222Z","iopub.status.idle":"2024-08-26T19:55:02.781868Z","shell.execute_reply.started":"2024-08-26T19:55:02.468174Z","shell.execute_reply":"2024-08-26T19:55:02.780658Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the Trainer with the custom loss function\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Your training dataset\n    eval_dataset=test_dataset  # Your evaluation dataset\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:04:19.194429Z","iopub.execute_input":"2024-08-26T20:04:19.194955Z","iopub.status.idle":"2024-08-26T20:04:19.212353Z","shell.execute_reply.started":"2024-08-26T20:04:19.194906Z","shell.execute_reply":"2024-08-26T20:04:19.210873Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:22:47.467830Z","iopub.execute_input":"2024-08-26T20:22:47.468155Z","iopub.status.idle":"2024-08-26T20:22:47.849977Z","shell.execute_reply.started":"2024-08-26T20:22:47.468118Z","shell.execute_reply":"2024-08-26T20:22:47.848376Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}